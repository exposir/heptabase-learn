<!--
- [INPUT]: 前端 Blob 操作、Spark-MD5、AbortController、ReadableStream
- [OUTPUT]: 针对 GB 级文件上传与下载的工业级解决方案
- [POS]: 编程题/ 模块的核心专题，解决浏览器环境下的文件 I/O 极限挑战
- [PROTOCOL]: 变更时更新此头部，然后检查 CLAUDE.md
-->

# 2.4 文件处理场景深度剖析

在浏览器环境下处理大文件（GB 级）是前端工程能力的试金石。**文件处理的本质是：如何绕过单线程的内存限制，通过"分治法（分片）"与"流式处理（Streaming）"在不可靠的网络环境中建立稳健的数据通道。**

---

### 2.4.1 大文件分片上传 (Large File Chunk Upload) ⭐⭐⭐⭐

**现象**：直接上传大文件会导致请求超时、内存溢出，且一旦网络波动必须从头再来。
**本质**：利用 `Blob.slice` 将大文件物理切割为多个 Chunk，并发上传，服务端最后进行物理合并。

#### 工业级功能清单与实现

1.  **文件 Hash 计算 (Fingerprinting)**：
    - **难点**：直接读取 2GB 文件计算 MD5 会导致 UI 卡死。
    - **方案**：**Worker + 增量计算 (Incremental Hashing)**。利用 `spark-md5` 的 `append` 方法分块读取并计算，不占用主线程。
2.  **断点续传 (Resumable Upload)**：
    - **方案**：上传前先调接口查询"已上传分片索引列表"。仅上传剩余分片。
3.  **并发控制 (Concurrency Control)**：
    - **方案**：使用 Promise 池限制并发数（如同时只传 3-6 个分片），防止撑爆浏览器 TCP 连接数。
4.  **秒传 (Instant Upload)**：
    - **方案**：若服务端已存在该文件的 Hash，直接返回成功，不触发实际上传。
5.  **暂停与恢复**：
    - **方案**：利用 `AbortController` 信号中断正在进行的 Fetch 请求。

#### 代码实现：工业级大文件分片上传引擎

> 哲学本质：**分治法在 I/O 维度的终极体现。** 一个 2GB 的文件是不可控的庞然大物，但 400 个 5MB 的分片是可以精确调度、重试、恢复的原子单元。控制粒度就是控制命运。

```typescript
/**
 * ChunkUploader — 大文件分片上传引擎
 *
 * 完整生命周期：
 * 选择文件 → Hash 计算（Worker）→ 秒传检查 → 断点续传查询
 * → 分片切割 → 并发上传（含暂停/恢复/重试）→ 合并通知
 */

// ================ 1. 并发控制池 ================

/**
 * ConcurrencyPool — Promise 并发限制器
 *
 * 核心算法：
 * 维护一个运行中任务集合，当数量达上限时，
 * 新任务等待（await Promise.race）直到有空位。
 */
class ConcurrencyPool {
  private running = new Set<Promise<unknown>>();

  constructor(private maxConcurrency: number) {}

  async add<T>(task: () => Promise<T>): Promise<T> {
    // 池满了 → 等待最先完成的任务腾出位置
    while (this.running.size >= this.maxConcurrency) {
      await Promise.race(this.running);
    }

    const promise = task().finally(() => {
      this.running.delete(promise);
    });

    this.running.add(promise);
    return promise;
  }

  /** 等待所有任务完成 */
  async flush(): Promise<void> {
    while (this.running.size > 0) {
      await Promise.race(this.running);
    }
  }
}

// ================ 2. Worker Hash 计算 ================

/**
 * 文件 Hash 计算 — 增量式 + Worker 隔离
 *
 * 为什么不能在主线程算？
 * → 2GB 文件 = 2GB 内存加载 + 数秒纯 CPU 计算 = 页面彻底冻结
 *
 * 策略：分块读取 + spark-md5 增量 append + Worker 线程
 */

// --- hash.worker.ts（Worker 线程内部代码）---
/*
importScripts('https://cdn.jsdelivr.net/npm/spark-md5@3.0.0/spark-md5.min.js');

self.onmessage = async (e: MessageEvent) => {
  const { chunks } = e.data as { chunks: Blob[] };
  const spark = new SparkMD5.ArrayBuffer();

  for (let i = 0; i < chunks.length; i++) {
    const buffer = await chunks[i].arrayBuffer();
    spark.append(buffer);

    // 向主线程报告进度
    self.postMessage({
      type: 'progress',
      percent: Math.round(((i + 1) / chunks.length) * 100),
    });
  }

  self.postMessage({
    type: 'done',
    hash: spark.end(),
  });
};
*/

// --- 主线程调用封装 ---
function calculateFileHash(
  file: File,
  chunkSize: number,
  onProgress?: (percent: number) => void,
): Promise<string> {
  return new Promise((resolve, reject) => {
    const chunks: Blob[] = [];
    let offset = 0;

    while (offset < file.size) {
      chunks.push(file.slice(offset, offset + chunkSize));
      offset += chunkSize;
    }

    const worker = new Worker(new URL("./hash.worker.ts", import.meta.url), {
      type: "module",
    });

    worker.onmessage = (e) => {
      if (e.data.type === "progress") {
        onProgress?.(e.data.percent);
      } else if (e.data.type === "done") {
        resolve(e.data.hash);
        worker.terminate();
      }
    };

    worker.onerror = (e) => {
      reject(new Error(e.message));
      worker.terminate();
    };

    worker.postMessage({ chunks });
  });
}

// ================ 3. 分片上传引擎 ================

interface UploadOptions {
  file: File;
  chunkSize?: number; // 默认 5MB
  concurrency?: number; // 并发数，默认 4
  maxRetries?: number; // 单片最大重试数
  onHashProgress?: (pct: number) => void;
  onUploadProgress?: (uploaded: number, total: number) => void;
}

interface ChunkMeta {
  index: number;
  blob: Blob;
  hash: string;
}

class ChunkUploader {
  private abortController: AbortController | null = null;
  private paused = false;
  private uploadedIndices = new Set<number>();

  async upload(opts: UploadOptions): Promise<{ url: string }> {
    const {
      file,
      chunkSize = 5 * 1024 * 1024, // 5 MB
      concurrency = 4,
      maxRetries = 3,
      onHashProgress,
      onUploadProgress,
    } = opts;

    // ========== STEP 1: 计算文件 Hash ==========
    const fileHash = await calculateFileHash(file, chunkSize, onHashProgress);

    // ========== STEP 2: 秒传检查 ==========
    const instantResult = await this.checkInstantUpload(fileHash);
    if (instantResult) {
      return instantResult; // 服务端已有此文件，直接返回
    }

    // ========== STEP 3: 断点续传 — 查询已上传分片 ==========
    this.uploadedIndices = await this.queryUploadedChunks(fileHash);

    // ========== STEP 4: 切片 ==========
    const totalChunks = Math.ceil(file.size / chunkSize);
    const chunks: ChunkMeta[] = [];

    for (let i = 0; i < totalChunks; i++) {
      if (this.uploadedIndices.has(i)) continue; // 跳过已上传分片

      const start = i * chunkSize;
      const end = Math.min(start + chunkSize, file.size);
      chunks.push({
        index: i,
        blob: file.slice(start, end),
        hash: fileHash,
      });
    }

    // ========== STEP 5: 并发上传 ==========
    this.abortController = new AbortController();
    const pool = new ConcurrencyPool(concurrency);
    let uploaded = this.uploadedIndices.size;

    const uploadChunkWithRetry = async (chunk: ChunkMeta) => {
      // 暂停检查 — 等待恢复信号
      while (this.paused) {
        await new Promise((r) => setTimeout(r, 200));
      }

      for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
          await this.uploadSingleChunk(chunk, this.abortController!.signal);
          this.uploadedIndices.add(chunk.index);
          uploaded++;
          onUploadProgress?.(uploaded, totalChunks);
          return;
        } catch (err) {
          if ((err as Error).name === "AbortError") throw err;
          if (attempt === maxRetries - 1) throw err;
          // 指数退避重试
          await new Promise((r) => setTimeout(r, 1000 * Math.pow(2, attempt)));
        }
      }
    };

    // 并发提交所有分片
    await Promise.all(
      chunks.map((chunk) => pool.add(() => uploadChunkWithRetry(chunk))),
    );

    // ========== STEP 6: 通知服务端合并 ==========
    return this.mergeChunks(fileHash, totalChunks, file.name);
  }

  /** 暂停上传 — 不中断已发出的请求，只阻塞后续分片 */
  pause() {
    this.paused = true;
  }

  /** 恢复上传 */
  resume() {
    this.paused = false;
  }

  /** 取消上传 — 中断所有正在进行的请求 */
  abort() {
    this.abortController?.abort();
  }

  // ============ 私有方法：与服务端交互 ============

  private async checkInstantUpload(hash: string) {
    const res = await fetch("/api/upload/check", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ hash }),
    });
    const data = await res.json();
    return data.exists ? { url: data.url } : null;
  }

  private async queryUploadedChunks(hash: string): Promise<Set<number>> {
    const res = await fetch(`/api/upload/progress?hash=${hash}`);
    const data = await res.json();
    return new Set(data.uploadedIndices as number[]);
  }

  private async uploadSingleChunk(chunk: ChunkMeta, signal: AbortSignal) {
    const form = new FormData();
    form.append("file", chunk.blob);
    form.append("hash", chunk.hash);
    form.append("index", String(chunk.index));

    await fetch("/api/upload/chunk", {
      method: "POST",
      body: form,
      signal,
    });
  }

  private async mergeChunks(hash: string, total: number, filename: string) {
    const res = await fetch("/api/upload/merge", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ hash, total, filename }),
    });
    return res.json();
  }
}

// ========== 使用示例 ==========
const input = document.querySelector<HTMLInputElement>("#file-input")!;

input.addEventListener("change", async () => {
  const file = input.files?.[0];
  if (!file) return;

  const uploader = new ChunkUploader();

  // 暂停按钮
  document.getElementById("pause")!.onclick = () => uploader.pause();
  document.getElementById("resume")!.onclick = () => uploader.resume();
  document.getElementById("cancel")!.onclick = () => uploader.abort();

  const result = await uploader.upload({
    file,
    chunkSize: 5 * 1024 * 1024,
    concurrency: 4,
    onHashProgress: (pct) => console.log(`Hash 计算: ${pct}%`),
    onUploadProgress: (done, total) =>
      console.log(`上传进度: ${done}/${total}`),
  });

  console.log("上传完成:", result.url);
});
```

**关键设计决策**：

- **为什么选 5MB 分片？** 太小导致 HTTP 开销占比过大；太大导致单片重传代价高。5MB 是经验平衡点。
- **为什么并发数限 4？** 浏览器同域并发连接限制（Chrome 约 6 个），留 2 个给页面其他请求。
- **为什么暂停不用 AbortController？** 暂停是"暂停发送新分片"，不应中断已在传输中的分片。取消才需要 Abort。

---

### 2.4.2 文件下载场景 (Advanced Download) ⭐⭐⭐

**现象**：点击链接直接下载无法显示进度，且对于大文件容易因内存不足而失败。
**本质**：从"全量加载"转向"流式处理"。

#### 实现要点

- **进度感知**：利用 `fetch` 的 `response.body.getReader()`。通过 `ReadableStream` 逐步读取 `Uint8Array`，并根据 `Content-Length` 计算百分比。
- **断点续传下载**：请求头携带 `Range: bytes=start-end`，服务端配合返回 `206 Partial Content`。
- **流式保存 (Stream Saving)**：使用 `StreamSaver.js` 或 `FileSystemWritableFileStream` 直接将流写入硬盘，不占用浏览器堆内存，从而支持下载 GB 级文件而不崩溃。

#### 代码实现：带进度与断点续传的下载引擎

> 哲学本质：**数据应像流水一样经过浏览器，而不应像湖泊一样停留在内存里。** `ReadableStream` 就是管道——数据从网络涌入，立刻写入磁盘，浏览器只是中转站，峰值内存永远只有一个 chunk 的大小。

```typescript
/**
 * StreamDownloader — 流式下载引擎
 *
 * 三种下载策略对比：
 * 1. Blob 下载     — 全量加载到内存，适合小文件（< 100MB）
 * 2. Stream 下载   — 流式写入磁盘，适合大文件（> 100MB）
 * 3. Range 下载    — 断点续传，适合不稳定网络
 */

// ================ 1. 带进度的 Fetch 封装 ================

interface DownloadProgress {
  loaded: number;
  total: number; // -1 if unknown
  percent: number; // 0-100, -1 if unknown
}

async function fetchWithProgress(
  url: string,
  onProgress: (p: DownloadProgress) => void,
  signal?: AbortSignal,
  rangeStart?: number,
): Promise<ReadableStream<Uint8Array>> {
  const headers: HeadersInit = {};
  if (rangeStart !== undefined && rangeStart > 0) {
    headers["Range"] = `bytes=${rangeStart}-`;
  }

  const response = await fetch(url, { headers, signal });

  if (!response.ok && response.status !== 206) {
    throw new Error(`Download failed: ${response.status}`);
  }

  const reader = response.body!.getReader();

  // ---- 解析总大小 ----
  const contentLength = response.headers.get("Content-Length");
  const total = contentLength ? parseInt(contentLength, 10) : -1;
  let loaded = 0;

  // ---- 创建带进度追踪的 Transform Stream ----
  return new ReadableStream({
    async pull(controller) {
      const { done, value } = await reader.read();

      if (done) {
        controller.close();
        return;
      }

      loaded += value.byteLength;
      onProgress({
        loaded,
        total,
        percent: total > 0 ? Math.round((loaded / total) * 100) : -1,
      });

      controller.enqueue(value);
    },

    cancel() {
      reader.cancel();
    },
  });
}

// ================ 2. 小文件下载（Blob 方案） ================

/**
 * 适用：< 100MB
 * 原理：全部读入内存 → 构造 Blob → 创建 Object URL → 触发 <a> 下载
 */
async function downloadAsBlob(
  url: string,
  filename: string,
  onProgress?: (p: DownloadProgress) => void,
) {
  const stream = await fetchWithProgress(url, onProgress ?? (() => {}));

  // 将流收集为 Blob
  const response = new Response(stream);
  const blob = await response.blob();

  // 触发浏览器下载
  const anchor = document.createElement("a");
  anchor.href = URL.createObjectURL(blob);
  anchor.download = filename;
  anchor.click();

  // 清理内存
  setTimeout(() => URL.revokeObjectURL(anchor.href), 1000);
}

// ================ 3. 大文件流式下载 ================

/**
 * 适用：> 100MB ~ GB 级
 * 原理：使用 File System Access API 直接将流写入磁盘
 *
 * 关键优势：
 * 浏览器内存峰值 ≈ 一个 chunk 的大小（≈64KB）
 * 而非整个文件大小
 */
async function downloadAsStream(
  url: string,
  filename: string,
  onProgress?: (p: DownloadProgress) => void,
) {
  // ---- 请求用户选择保存位置 ----
  const fileHandle = await window.showSaveFilePicker({
    suggestedName: filename,
  });
  const writable = await fileHandle.createWritable();

  const abortController = new AbortController();

  try {
    const stream = await fetchWithProgress(
      url,
      onProgress ?? (() => {}),
      abortController.signal,
    );

    const reader = stream.getReader();

    // ---- 逐 chunk 写入磁盘，不累积 ----
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      await writable.write(value); // 直接写入文件系统
    }

    await writable.close();
  } catch (err) {
    await writable.abort();
    throw err;
  }

  return { cancel: () => abortController.abort() };
}

// ================ 4. 断点续传下载 ================

/**
 * 适用：不稳定网络 + 大文件
 * 原理：
 * 1. 使用 OPFS (Origin Private File System) 暂存已下载部分
 * 2. 中断后重新请求时携带 Range 头，从中断处继续
 * 3. 下载完成后从 OPFS 移到用户可见目录
 */
class ResumableDownloader {
  private abortController: AbortController | null = null;
  private downloadedBytes = 0;

  async download(
    url: string,
    filename: string,
    onProgress?: (p: DownloadProgress) => void,
  ) {
    // ---- 检查 OPFS 中是否有未完成的下载 ----
    const opfsRoot = await navigator.storage.getDirectory();
    const tempHandle = await opfsRoot.getFileHandle(
      `download_${this.hashUrl(url)}`,
      { create: true },
    );

    const tempFile = await tempHandle.getFile();
    this.downloadedBytes = tempFile.size;

    const writable = await tempHandle.createWritable({
      keepExistingData: true, // 关键：保留已下载内容
    });

    // 移动写入位置到已下载的末尾
    await writable.seek(this.downloadedBytes);

    this.abortController = new AbortController();

    try {
      const stream = await fetchWithProgress(
        url,
        (p) => {
          onProgress?.({
            loaded: this.downloadedBytes + p.loaded,
            total: p.total > 0 ? this.downloadedBytes + p.total : -1,
            percent:
              p.total > 0
                ? Math.round(
                    ((this.downloadedBytes + p.loaded) /
                      (this.downloadedBytes + p.total)) *
                      100,
                  )
                : -1,
          });
        },
        this.abortController.signal,
        this.downloadedBytes,
      );

      const reader = stream.getReader();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        await writable.write(value);
      }

      await writable.close();

      // ---- 下载完成：从 OPFS 导出到用户目录 ----
      await this.exportFromOPFS(tempHandle, filename);
      await opfsRoot.removeEntry(tempHandle.name);
    } catch (err) {
      await writable.close(); // 保留已下载部分
      if ((err as Error).name !== "AbortError") throw err;
      console.log(`已暂停，已下载 ${this.downloadedBytes} 字节，下次可续传`);
    }
  }

  pause() {
    this.abortController?.abort();
  }

  private async exportFromOPFS(handle: FileSystemFileHandle, filename: string) {
    const file = await handle.getFile();
    const saveHandle = await window.showSaveFilePicker({
      suggestedName: filename,
    });
    const writable = await saveHandle.createWritable();
    await file.stream().pipeTo(writable);
  }

  private hashUrl(url: string): string {
    // 简单哈希，生产环境用 crypto.subtle
    let hash = 0;
    for (const ch of url) {
      hash = ((hash << 5) - hash + ch.charCodeAt(0)) | 0;
    }
    return Math.abs(hash).toString(36);
  }
}

// ========== 使用示例 ==========

// 小文件直接下载
await downloadAsBlob("/api/files/report.pdf", "report.pdf", (p) =>
  console.log(`下载进度: ${p.percent}%`),
);

// 大文件流式下载
await downloadAsStream(
  "/api/files/dataset.zip", // 2GB
  "dataset.zip",
  (p) => console.log(`流式下载: ${p.percent}%`),
);

// 不稳定网络 — 断点续传
const downloader = new ResumableDownloader();
document.getElementById("pause")!.onclick = () => downloader.pause();
await downloader.download(
  "/api/files/huge-model.bin", // 10GB
  "model.bin",
  (p) => console.log(`续传下载: ${p.loaded} / ${p.total}`),
);
```

**三种方案选择决策树**：

- 文件 < 100MB → `downloadAsBlob`（简单可靠）
- 文件 > 100MB → `downloadAsStream`（流式，零内存积累）
- 网络不稳定 + 大文件 → `ResumableDownloader`（OPFS 暂存 + Range 续传）

---

### 2.4.3 前端 Excel/CSV 解析与导出 ⭐⭐⭐

**现象**：将 10 万行 Excel 一次性读入内存会导致页面崩溃（OOM）。
**本质**：**流式解析与 Worker 隔离。**

#### 解决方案

1.  **解析策略**：使用 `SheetJS` (xlsx) 的流式 API。在 **Web Worker** 中执行解析，避免阻塞渲染。
2.  **数据校验**：在解析过程中按行校验（Streaming Validation），而非全部解析完再校验。
3.  **导出优化**：对于超大规模数据导出，建议使用 CSV 格式以减少库的计算开销，或在后端生成下载链接。

#### 代码实现：流式 CSV 解析与高性能导出

> 哲学本质：**解析和导出是文件处理的"读/写"两端。** 解析要"流式"——数据经过就处理，不积累；导出要"批量"——攒够一批再写入，减少 I/O 次数。两者看似矛盾，实则殊途同归：都是在内存与速度之间寻找最优平衡点。

```typescript
/**
 * 前端 CSV/Excel 处理工具集
 *
 * 为什么优先用 CSV 而非 xlsx？
 * 1. CSV 是纯文本，可流式解析，内存可控
 * 2. xlsx 是 ZIP 压缩包，必须全量解压，内存不可控
 * 3. 10 万行 CSV ≈ 20MB，10 万行 xlsx ≈ 50MB + 解压内存
 *
 * 结论：能用 CSV 就不用 xlsx，这不是偷懒，是架构选择
 */

// ================ 1. 流式 CSV 解析器 ================

/**
 * StreamCSVParser — 边读边解析，峰值内存 = 1 行
 *
 * 支持：
 * - RFC 4180 标准（引号内换行、转义引号）
 * - 逐行回调，不累积
 * - Web Worker 隔离
 */
interface CSVParseOptions {
  delimiter?: string; // 默认 ','
  hasHeader?: boolean; // 首行是否为标题
  encoding?: string; // 文件编码，默认 UTF-8
  onRow: (row: string[], index: number) => void;
  onHeader?: (headers: string[]) => void;
  onError?: (error: Error, lineNumber: number) => void;
  onComplete?: (stats: { totalRows: number; errors: number }) => void;
}

class StreamCSVParser {
  async parse(file: File, options: CSVParseOptions) {
    const {
      delimiter = ",",
      hasHeader = true,
      encoding = "utf-8",
      onRow,
      onHeader,
      onError,
      onComplete,
    } = options;

    const decoder = new TextDecoder(encoding);
    const reader = file.stream().getReader();

    let buffer = ""; // 未处理的残余文本
    let lineNumber = 0;
    let errorCount = 0;
    let headers: string[] | null = null;

    const processLine = (line: string) => {
      try {
        const fields = this.parseLine(line, delimiter);

        if (hasHeader && !headers) {
          headers = fields;
          onHeader?.(headers);
          return;
        }

        lineNumber++;
        onRow(fields, lineNumber);
      } catch (err) {
        errorCount++;
        onError?.(err as Error, lineNumber);
      }
    };

    // ---- 流式读取 ----
    while (true) {
      const { done, value } = await reader.read();

      if (done) {
        // 处理最后一行（可能没有换行符结尾）
        if (buffer.trim()) {
          processLine(buffer);
        }
        break;
      }

      buffer += decoder.decode(value, { stream: true });

      // 按换行符切割，但要跳过引号内的换行
      const lines = this.splitLines(buffer);

      // 最后一个元素可能是不完整的行，留到下次
      buffer = lines.pop()!;

      for (const line of lines) {
        processLine(line);
      }
    }

    onComplete?.({ totalRows: lineNumber, errors: errorCount });
  }

  /**
   * 解析单行 CSV — 处理引号内的逗号和换行
   *
   * RFC 4180 核心规则：
   * 1. 字段包含分隔符/换行/引号时，必须用双引号包裹
   * 2. 引号内的引号用两个引号转义：""
   */
  private parseLine(line: string, delimiter: string): string[] {
    const fields: string[] = [];
    let current = "";
    let inQuotes = false;
    let i = 0;

    while (i < line.length) {
      const ch = line[i];

      if (inQuotes) {
        if (ch === '"') {
          // 连续两个引号 → 转义的引号
          if (line[i + 1] === '"') {
            current += '"';
            i += 2;
          } else {
            inQuotes = false;
            i++;
          }
        } else {
          current += ch;
          i++;
        }
      } else {
        if (ch === '"') {
          inQuotes = true;
          i++;
        } else if (ch === delimiter) {
          fields.push(current);
          current = "";
          i++;
        } else {
          current += ch;
          i++;
        }
      }
    }

    fields.push(current); // 最后一个字段
    return fields;
  }

  /** 按换行分割，但忽略引号内的换行 */
  private splitLines(text: string): string[] {
    const lines: string[] = [];
    let current = "";
    let inQuotes = false;

    for (const ch of text) {
      if (ch === '"') inQuotes = !inQuotes;

      if ((ch === "\n" || ch === "\r") && !inQuotes) {
        if (ch === "\r") continue; // 跳过 \r，等 \n
        lines.push(current);
        current = "";
      } else {
        current += ch;
      }
    }

    lines.push(current); // 残余部分
    return lines;
  }
}

// ================ 2. 高性能 CSV 导出 ================

/**
 * CSVExporter — 流式导出 + Blob 构建
 *
 * 为什么不用字符串拼接？
 * → 100 万个字符串 += 操作会触发大量 GC
 * → 用数组 push + 最终 join 或 Blob 构建更高效
 */
class CSVExporter {
  /**
   * 导出为 CSV 文件 — 小数据量（< 10 万行）
   */
  static exportSmall(
    headers: string[],
    rows: (string | number | null)[][],
    filename: string,
  ) {
    // BOM 头 — 确保 Windows Excel 正确识别 UTF-8
    const BOM = "\uFEFF";

    const csvParts: string[] = [
      BOM,
      headers.map((h) => this.escapeField(h)).join(","),
      "\n",
    ];

    for (const row of rows) {
      csvParts.push(
        row.map((cell) => this.escapeField(String(cell ?? ""))).join(","),
      );
      csvParts.push("\n");
    }

    const blob = new Blob(csvParts, { type: "text/csv;charset=utf-8" });
    this.triggerDownload(blob, filename);
  }

  /**
   * 流式导出 — 大数据量（> 10 万行）
   *
   * 使用 File System Access API，
   * 边生成边写入磁盘，内存中不积累完整文件
   */
  static async exportLarge(
    headers: string[],
    rowGenerator: AsyncGenerator<(string | number | null)[]>,
    filename: string,
    onProgress?: (count: number) => void,
  ) {
    const handle = await window.showSaveFilePicker({
      suggestedName: filename,
      types: [{ accept: { "text/csv": [".csv"] } }],
    });

    const writable = await handle.createWritable();
    const encoder = new TextEncoder();

    // 写 BOM + 表头
    await writable.write(
      encoder.encode(
        "\uFEFF" + headers.map((h) => this.escapeField(h)).join(",") + "\n",
      ),
    );

    let count = 0;
    const BATCH_SIZE = 1000;
    let batch = "";

    for await (const row of rowGenerator) {
      batch +=
        row.map((c) => this.escapeField(String(c ?? ""))).join(",") + "\n";

      count++;

      // 每 1000 行写入一次，平衡 I/O 次数和内存占用
      if (count % BATCH_SIZE === 0) {
        await writable.write(encoder.encode(batch));
        batch = "";
        onProgress?.(count);
      }
    }

    // 写入最后不足 BATCH_SIZE 的部分
    if (batch) {
      await writable.write(encoder.encode(batch));
    }

    await writable.close();
    onProgress?.(count);
  }

  /** 字段转义 — 包含特殊字符时用双引号包裹 */
  private static escapeField(field: string): string {
    if (field.includes(",") || field.includes('"') || field.includes("\n")) {
      return `"${field.replace(/"/g, '""')}"`;
    }
    return field;
  }

  private static triggerDownload(blob: Blob, filename: string) {
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = filename;
    a.click();
    setTimeout(() => URL.revokeObjectURL(url), 1000);
  }
}

// ================ 3. Worker 中解析 Excel (xlsx) ================

/**
 * Excel 解析必须在 Worker 中执行
 *
 * 因为 xlsx 文件本质是 ZIP 包，
 * 解压 + 解析 XML 是 CPU 密集 + 内存密集双杀操作
 */

// --- excel.worker.ts ---
/*
importScripts(
  'https://cdn.sheetjs.com/xlsx-latest/package/dist/xlsx.full.min.js'
);

self.onmessage = async (e: MessageEvent) => {
  const { buffer, sheetIndex = 0 } = e.data;

  try {
    const workbook = XLSX.read(buffer, { type: 'array' });
    const sheetName = workbook.SheetNames[sheetIndex];
    const sheet = workbook.Sheets[sheetName];

    // 按行生成 JSON，不要一次性 sheet_to_json
    const range = XLSX.utils.decode_range(sheet['!ref']!);
    const headers: string[] = [];

    for (let col = range.s.c; col <= range.e.c; col++) {
      const cell = sheet[XLSX.utils.encode_cell({ r: range.s.r, c: col })];
      headers.push(cell?.v?.toString() ?? `Column_${col}`);
    }

    self.postMessage({ type: 'headers', headers });

    // 逐行发送，避免一次性传回巨量数据
    for (let row = range.s.r + 1; row <= range.e.r; row++) {
      const rowData: (string | number | null)[] = [];
      for (let col = range.s.c; col <= range.e.c; col++) {
        const cell = sheet[XLSX.utils.encode_cell({ r: row, c: col })];
        rowData.push(cell?.v ?? null);
      }
      self.postMessage({ type: 'row', data: rowData, index: row });
    }

    self.postMessage({
      type: 'done',
      totalRows: range.e.r - range.s.r,
    });
  } catch (err) {
    self.postMessage({
      type: 'error',
      message: (err as Error).message,
    });
  }
};
*/

// --- 主线程调用封装 ---
function parseExcelInWorker(
  file: File,
  callbacks: {
    onHeader: (headers: string[]) => void;
    onRow: (row: (string | number | null)[], index: number) => void;
    onDone: (total: number) => void;
    onError: (err: Error) => void;
  },
) {
  const worker = new Worker(new URL("./excel.worker.ts", import.meta.url), {
    type: "module",
  });

  // 读取文件为 ArrayBuffer 后发送给 Worker
  file.arrayBuffer().then((buffer) => {
    worker.postMessage({ buffer }, [buffer]); // Transferable
  });

  worker.onmessage = (e) => {
    switch (e.data.type) {
      case "headers":
        callbacks.onHeader(e.data.headers);
        break;
      case "row":
        callbacks.onRow(e.data.data, e.data.index);
        break;
      case "done":
        callbacks.onDone(e.data.totalRows);
        worker.terminate();
        break;
      case "error":
        callbacks.onError(new Error(e.data.message));
        worker.terminate();
        break;
    }
  };
}

// ========== 使用示例 ==========

// ---- 解析 CSV ----
const csvParser = new StreamCSVParser();
await csvParser.parse(file, {
  hasHeader: true,
  onHeader: (h) => console.log("表头:", h),
  onRow: (row, i) => {
    // 逐行处理，可以在这里做校验、入库等
    if (i % 10000 === 0) console.log(`已解析 ${i} 行`);
  },
  onComplete: (stats) =>
    console.log(`完成: ${stats.totalRows} 行, ${stats.errors} 个错误`),
});

// ---- 导出小规模 CSV ----
CSVExporter.exportSmall(
  ["姓名", "年龄", "城市"],
  [
    ["张三", 28, "北京"],
    ["李四", 32, "上海"],
  ],
  "users.csv",
);

// ---- 流式导出大规模 CSV ----
async function* generateRows() {
  for (let i = 0; i < 1_000_000; i++) {
    yield [`User_${i}`, Math.floor(Math.random() * 60), `City_${i % 100}`];
  }
}

await CSVExporter.exportLarge(
  ["Name", "Age", "City"],
  generateRows(),
  "million_users.csv",
  (count) => console.log(`已导出 ${count} 行`),
);
```

**CSV vs Excel 选型指导**：
| 维度 | CSV | Excel (xlsx) |
| :--- | :--- | :--- |
| **内存控制** | ✅ 可流式，峰值 = 1 行 | ❌ 必须全量解压 |
| **解析速度** | ✅ 纯文本，极快 | ❌ ZIP 解压 + XML 解析 |
| **富格式** | ❌ 纯数据 | ✅ 样式、公式、图表 |
| **推荐场景** | 数据导入导出、ETL | 财务报表、复杂模板 |

---

## 哲学启示

文件处理遵循 **"不累积 (No Accumulation)"** 原则：数据应像流水一样经过浏览器，而不应像湖泊一样停留在内存里。分片是时间的拆解，流是空间的节约。

**三层认知跃迁**：

1. **现象层（How to fix）**：用 `Blob.slice` 分片、用 `ReadableStream` 读流、用 Worker 隔离计算。
2. **本质层（Why it breaks）**：浏览器堆内存有上限（通常 2-4GB），单次 `arrayBuffer()` 加载 2GB 文件就是自杀行为。
3. **哲学层（How to design it right）**：**不持有 > 临时持有 > 长期持有。** 理想的文件处理架构中，数据从未在内存中"停留"过——它只是"流过"。
